:mod:`data_scout.executor`
==========================

.. py:module:: data_scout.executor


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   data_scout.executor.Executor
   data_scout.executor.PandasExecutor
   data_scout.executor.CodeExecutor
   data_scout.executor.SparkExecutor




.. class:: Executor(data_source: dict, pipeline: List[dict], scout: data_scout.scout.Scout)


   This is the abstract executor. It contains the base method every executor should implement.

   .. method:: join(self, left, right, on_left: List[str], on_right: List[str], how: str)
      :abstractmethod:


   .. method:: load_data(self, use_sample: bool = False, sampling_technique: str = 'top') -> List[dict]


   .. method:: _make_dataframe(self, records: List[dict])
      :abstractmethod:

      Make a dataframe.
      :param records: The input records (list for Pandas, RDD for PySpark)
      :return:


   .. method:: _fix_missing_columns(self, records: List[dict])
      :abstractmethod:

      Make sure that each row in the dataset has the same keys.
      :param records: The input records (list for Pandas, RDD for PySpark)
      :return:


   .. method:: _get_columns(self, records: List[dict]) -> Tuple[dict, Any]
      :abstractmethod:

      Get a list of column_name: column_type dicts.

      :param records: A list of all records
      :return: A list of column names and their types and a complete dataframe


   .. method:: _apply(self, records, transformation: data_scout.transformations.transformation.Transformation)
      :abstractmethod:


   .. method:: _apply_flatten(self, records, transformation: data_scout.transformations.transformation.Transformation)
      :abstractmethod:

      Apply a function that expands the records

      :param records:
      :return:


   .. method:: _apply_global(self, records, transformation: data_scout.transformations.transformation.Transformation)
      :abstractmethod:

      Apply a function to all the records (group bys, etc.)

      :param records:
      :return:


   .. method:: _filter(self, records)
      :abstractmethod:

      Filter all elements that are False

      :param records:
      :return:


   .. method:: _get_transformations(self) -> List[Tuple[int, dict, Type[data_scout.transformations.transformation.Transformation]]]


   .. method:: _get_sampling_technique(self, requested_technique: str, transformation_list: List[Tuple[int, dict, Type[data_scout.transformations.transformation.Transformation]]])


   .. method:: __call__(self, use_sample: bool = True, sampling_technique: str = 'top', column_types: bool = False)

      Execute the pipeline that this executor was initialized with.

      :param use_sample: Should the data be sampled?
      :param sampling_technique: What sampling technique to use (only if use_sample is true)?
      :param column_types: Should the column types of all steps be returned? If not, an empty list is returned
      :return: A list of dictionary objects representing the data and a list of dicts representing the columns and
      column types.



.. class:: PandasExecutor(data_source: dict, pipeline: List[dict], scout: data_scout.scout.Scout)


   Bases: :py:obj:`Executor`

   Execute a pipeline in Pandas.

   .. method:: _apply(self, records, transformation: data_scout.transformations.transformation.Transformation)


   .. method:: _apply_global(self, df_records, transformation: data_scout.transformations.transformation.Transformation)

      Apply a function to all the records (group bys, etc.)

      :param records:
      :return:


   .. method:: _apply_flatten(self, records, transformation: data_scout.transformations.transformation.Transformation)

      Apply a function that expands the records

      :param records:
      :return:


   .. method:: _get_columns(self, records: List[dict]) -> Tuple[dict, pandas.DataFrame]

      Get a list of column_name: column_type dicts.

      :param records: A list of all records
      :return: A list of column names and their types and a complete dataframe


   .. method:: _fix_missing_columns(self, records)

      Make sure that each row in the dataset has the same keys.
      :param records: The input records (list for Pandas, RDD for PySpark)
      :return:


   .. method:: _make_dataframe(self, records: List[dict])

      Make a dataframe.
      :param records: The input records (list for Pandas, RDD for PySpark)
      :return:


   .. method:: _filter(self, records)

      Filter all elements that are False

      :param records:
      :return:



.. class:: CodeExecutor(data_source: dict, pipeline: List[dict], scout: data_scout.scout.Scout)


   Bases: :py:obj:`Executor`

   This is the abstract executor. It contains the base method every executor should implement.

   .. method:: _get_columns(self, records: List[dict]) -> Tuple[dict, Any]

      Get a list of column_name: column_type dicts.

      :param records: A list of all records
      :return: A list of column names and their types and a complete dataframe


   .. method:: load_data(self, use_sample: bool = False, sampling_technique: str = 'top') -> str


   .. method:: _apply(self, records, transformation: Optional[data_scout.transformations.transformation.Transformation]) -> str


   .. method:: _apply_global(self, df_records, transformation: Optional[data_scout.transformations.transformation.Transformation])

      Apply a function to all the records (group bys, etc.)

      :param records:
      :return:


   .. method:: _apply_flatten(self, records, transformation: Optional[data_scout.transformations.transformation.Transformation])

      Apply a function that expands the records

      :param records:
      :return:


   .. method:: _fix_missing_columns(self, records)

      Make sure that each row in the dataset has the same keys.
      :param records: The input records (list for Pandas, RDD for PySpark)
      :return:


   .. method:: _make_dataframe(self, records: List[dict])

      Make a dataframe.
      :param records: The input records (list for Pandas, RDD for PySpark)
      :return:


   .. method:: _filter(self, records)

      Filter all elements that are False

      :param records:
      :return:


   .. method:: _make_data_source(self)


   .. method:: _class_name(self, o)

      Get the fully classified class name of a certain type.

      :param o: The class
      :return:


   .. method:: __call__(self, use_sample: bool = True, sampling_technique: str = 'top', column_types: bool = False)

      Create a piece of code that will execute all of the steps in Pandas.

      :param use_sample: Not used
      :param sampling_technique: Not used
      :param column_types: Not used
      :return: A string containing the code and an empty list.



.. class:: SparkExecutor(data_source: dict, pipeline: List[dict], scout: data_scout.scout.Scout)


   Bases: :py:obj:`Executor`

   This is the abstract executor. It contains the base method every executor should implement.

   .. method:: _apply(self, records, transformation: data_scout.transformations.transformation.Transformation)


   .. method:: _apply_global(self, df_records, transformation: data_scout.transformations.transformation.Transformation)

      Apply a function to all the records (group bys, etc.)

      :param records:
      :return:


   .. method:: _apply_flatten(self, rdd, transformation: data_scout.transformations.transformation.Transformation)

      Apply a function that expands the records

      :param records:
      :return:


   .. method:: _get_columns(self, records: List[dict]) -> Tuple[dict, pandas.DataFrame]

      Get a list of column_name: column_type dicts.

      :param records: A list of all records
      :return: A list of column names and their types and a complete dataframe


   .. method:: _fix_missing_columns(self, records)

      Make sure that each row in the dataset has the same keys.
      :param records: The input records (list for Pandas, RDD for PySpark)
      :return:


   .. method:: _make_dataframe(self, records)

      Make a dataframe.
      :param records: The input records (list for Pandas, RDD for PySpark)
      :return:


   .. method:: _filter(self, records)

      Filter all elements that are False

      :param records:
      :return:



